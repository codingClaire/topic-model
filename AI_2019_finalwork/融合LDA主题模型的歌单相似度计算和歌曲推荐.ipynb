{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "【AI 2019选修课 大作业】  \n",
    "这个题目是我一直想探索的，借着人工智能大作业的选题把它提出来，也刚好也算是大创项目的一个子任务。     \n",
    "数据集来源于一位学长推荐的[课程](https://www.bilibili.com/video/av32279263?p=1)， 也借鉴了里面的部分思路。  \n",
    "最初的创新点本来是想结合歌词主题建模+协同过滤，但是在粤语歌主题建模中发现效果极差，一方面都是粤语歌相似性过高（这个经验恰好也用在了之后的主题建模任务中），另一方面，歌曲由于语言的差异等，反而不是很适合主题建模。但是我又想用主题建模，所以就利用了数据集里面的所有文本信息，然后发现其实效果还不错，于是就变成了歌单描述主题建模+协同过滤的整个思路。  \n",
    "然后比较麻烦的一点是没法用surprise库，看遍StackOverflow也没找到解决办法，所以也是借此机会自己写了一遍协同过滤（但程序的效率说明我太菜了..）。\n",
    "写着写着就大概理解了，这两者都是在运用集体智慧，还挺有意思，不过也还有很多没解决的事情。但对于大作业已经基本完整了。\n",
    "\n",
    "----\n",
    "\n",
    "\n",
    "#  1 数据预处理\n",
    "\n",
    "## 1.1 筛选需要的数据项\n",
    "\n",
    "首先对15G的歌单数据进行处理，选取了数据集中歌单的信息，包括歌单名、歌单标签、歌单id、歌单描述、歌单收藏数、歌曲信息，每一个歌单下包含着多首歌曲，每一首歌曲的信息包括歌曲id、歌曲名称、歌手、歌曲热度。 （15G的数据量较大，在jupyter跑不起来，python文件可以）  \n",
    "最终输出一个playlist_info的txt文件，经过收藏量的筛选，只剩下24951个歌单。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import pickle as pk\n",
    "import sys\n",
    "import json\n",
    "\n",
    "def readPlaylist(in_line):\n",
    "    data = json.loads(in_line)\n",
    "    name = data['result']['name'] #歌单名称\n",
    "    tags = \",\".join(data['result']['tags']) # 标签以逗号分隔的字符串存储\n",
    "    subscribed_count = data['result']['subscribedCount'] # 收藏数\n",
    "    try:\n",
    "        description=data['result']['description'] #歌单的描述\n",
    "    except Exception as e:\n",
    "        description=\" \"\n",
    "    if(subscribed_count<100): # 筛选小于100收藏的歌单\n",
    "        return False\n",
    "    playlist_id = data['result']['id'] # 歌单的id名\n",
    "    song_info = ''\n",
    "    songs = data['result']['tracks'] # 歌曲\n",
    "    for song in songs: \n",
    "        try:\n",
    "            song_info += \"\\t\"+\":::\".join([str(song['id']),song['name'],song['artists'][0]['name'],str(song['popularity'])])\n",
    "            song_info=song_info.replace(\"\\n\",\" \")\n",
    "            # 歌曲信息：歌曲id:::歌曲名称:::歌手:::歌曲热度\n",
    "        except Exception as e:\n",
    "            continue\n",
    "     # 歌单信息： 歌单名##歌单标签##歌单id##歌单描述##歌单收藏数##歌曲信息（以:::分割的信息）\n",
    "    final_info=str(name+\"##\"+tags+\"##\"+str(playlist_id)+\"##\"+str(description)+\"##\"+str(subscribed_count)+song_info)\n",
    "    return final_info.replace(\"\\n\",\" \")\n",
    "   \n",
    "def readFile(in_file, out_file):\n",
    "    out = open(out_file, 'w',encoding=\"utf-8\")\n",
    "    num=0\n",
    "    for line in open(in_file,encoding='utf-8'):\n",
    "        result = readPlaylist(line)\n",
    "        #print(result)\n",
    "        num=num+1\n",
    "        print(\"------------------\",num,\"--------------------\")\n",
    "        if(result):\n",
    "            out.write(str(result)+\"\\n\")  \n",
    "    out.close()\n",
    "    \n",
    "if __name__==\"__main__\":\n",
    "    readFile(\"./playlistdetail.all.json\", \"./playlist_info_utf8.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 构建歌曲（歌单）id和名称的字典\n",
    "\n",
    "在这一步构建了歌曲和歌单的两个属性id和名称对应的字典。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pk\n",
    "import sys\n",
    "\n",
    "def getDict(i, dic_playlist, dic_song):\n",
    "    contents = i.strip().split(\"\\t\")\n",
    "    playlist = contents[0].split(\"##\")\n",
    "    # 歌单信息： 歌单名##歌单标签##歌单id##歌单描述##歌单收藏数##歌曲信息（以:::分割的信息）\n",
    "    dic_playlist[playlist[2]] = playlist[0]\n",
    "    for i in contents[1:]:\n",
    "        try:\n",
    "            song_info = i.split(\":::\")\n",
    "            dic_song[song_info[0]] = song_info[1]+\"\\t\"+song_info[2]\n",
    "        except:\n",
    "            print(\"error\"+i+\"\\n\")\n",
    "\n",
    "\n",
    "def readFile(in_file, out_playlist, out_song):\n",
    "    dic_playlist = {}  # dic_playlist[歌单id]=歌单名称\n",
    "    dic_song = {}  # dic_song[歌曲id]=歌曲名称\n",
    "    num = 0\n",
    "    for i in open(in_file, encoding=\"utf-8\"):\n",
    "        num = num+1\n",
    "        print(\"------------------\", num, \"--------------------\")\n",
    "        getDict(i, dic_playlist, dic_song)\n",
    "    pk.dump(dic_playlist, open(out_playlist, \"wb\"))\n",
    "    pk.dump(dic_song, open(out_song, \"wb\"))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    readFile(\"./playlist_info_utf8.txt\", \"playlist.pkl\", \"song.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 歌单文本信息分词（utf-8存储）\n",
    "为了后续的LDA主题模型，对歌单的文本信息进行分词。\n",
    "歌单的文本信息包括歌单名称、歌单标签和歌单描述。歌单的文本信息中除了中英文，还会有很多表情符（使用unicode编码），虽然表情符一定程度上可以表征歌单的情感以及歌曲的情感，当然要具体细究那就是后续的工作了，此处需要将这些表情符进行过滤。  \n",
    "1.将需要使用的文本信息进行提取，提取后存储在utf8_playlist.txt中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import pickle as pk\n",
    "import sys\n",
    "import json\n",
    "\n",
    "def readPlaylist(in_line):\n",
    "    data = json.loads(in_line)\n",
    "    name = data['result']['name'] #歌单名称\n",
    "    tags = \",\".join(data['result']['tags']) # 标签以逗号分隔的字符串存储\n",
    "    try:\n",
    "        description=data['result']['description'] #歌单的描述\n",
    "    except Exception as e:\n",
    "        description=\"---\"\n",
    "    description=(str(description)).replace(\"\\n\",\"  \")\n",
    "    subscribed_count = data['result']['subscribedCount'] # 收藏数\n",
    "    if(subscribed_count<100):\n",
    "        return False\n",
    "    playlist_id = data['result']['id'] # 歌单的id名\n",
    "    return name+\"##\"+tags+\"##\"+str(playlist_id)+\"##\"+str(description)\n",
    "    # 歌单信息： 歌单名##歌单标签##歌单id##歌单描述\n",
    "\n",
    "def readFile(in_file, out_file):\n",
    "    out=open(out_file, 'w',encoding=\"utf-8\")\n",
    "    num=0\n",
    "    for line in open(in_file,encoding='utf-8'):\n",
    "        result=readPlaylist(line)\n",
    "        num=num+1\n",
    "        print(\"------------------\",num,\"--------------------\")\n",
    "        # print(result)\n",
    "        if(result):\n",
    "            out.write(str(result)+\"\\n\")  \n",
    "    out.close()\n",
    "    \n",
    "if __name__==\"__main__\":\n",
    "    readFile(\"D:\\playlistdetail.all.json\", \"./utf8_playlist.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.将所有的文本信息（标签、名称、描述）合在一起，由此汇成一个“文档”，这一个文档可认为是对歌单的总体描述，然后使用jieba分词，每一个文档分词的结果用list存储起来，最后歌单id和分词后的list以字典形式存储在dic_splited_sentence.pkl。  \n",
    "（这里也可以考虑加入歌单里的每一首歌的名称，甚至歌的歌词进行主题建模，但由于收集歌词的难度较大，就没有做到这一步）  \n",
    "由于歌单的描述中会混合非中文的其他语言文字或者一些颜文字、表情等，为了简便分词，将这些词都过滤，并且也根据导入的停用词表，过滤停用词，在这里会根据后面LDA模型的生成情况，添加一些针对歌单描述的特定停用词，如“歌曲”、“专辑”、“歌手”、“年度”、“单曲”、“歌单”等。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "import chardet\n",
    "import jieba\n",
    "import pickle as pk\n",
    "import re\n",
    "import emoji\n",
    "import pandas as pd\n",
    "\n",
    "def getSplitedSentence(i, dic_splited_sentence, stopwords):\n",
    "    # 获取歌单信息: 歌单名##歌单标签##歌单id##歌单描述\n",
    "    contents = i.strip().split(\"\\t\")\n",
    "    playlist = contents[0].split(\"##\")\n",
    "    sentence = \" \".join([playlist[0], playlist[1], playlist[3]])\n",
    "    chinese_sentence = re.sub('[^\\u4e00-\\u9fa5]', '', sentence)  # 过滤非中文\n",
    "    word_list = jieba.lcut(chinese_sentence)\n",
    "    word_list = filter(lambda x: len(x) > 1, word_list)\n",
    "    word_list = filter(lambda x: x not in stopwords, word_list)  # 过滤停用词\n",
    "    word_list = list(word_list)\n",
    "    # print(word_list)\n",
    "    dic_splited_sentence[playlist[2]] = word_list\n",
    "\n",
    "\n",
    "def readFile(in_file, out_pk, stopwords):\n",
    "    dic_splited_sentence = {}\n",
    "    num = 0\n",
    "    for i in open(in_file, encoding=\"utf-8\"):\n",
    "        print(\"------------\", num, \"-------------\")\n",
    "        num = num+1\n",
    "        getSplitedSentence(i, dic_splited_sentence, stopwords)\n",
    "    pk.dump(dic_splited_sentence, open(out_pk, \"wb\"))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    stopwords = pd.read_csv(\"./stopwords.txt\", index_col=False,\n",
    "                            quoting=3, sep=\"\\t\", names=['stopword'], encoding='utf-8')\n",
    "    stopwords = stopwords['stopword'].values\n",
    "    readFile(\"./playlist_info_utf8.txt\",\n",
    "             \"./dic_splited_sentence.pkl\", stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 歌单文本信息LDA主题建模\n",
    "LDA主题建模的思想是认为一篇文章的产生是服从概率分布的，每个词以一定概率选择了某个主题，每一个主题以一定概率选择了一篇文章。简单地说，LDA是一种主题抽取模型，通过无监督学习从文档中提出主题以及主题中的词语。最终LDA主题建模能够判断文档之间的相似性。因此可以考虑将歌单相关的文本信息都作为一个文档，通过LDA主题建模来进行歌单的聚类。  \n",
    "以下使用gensim库中的函数，进行LDA主题建模，实现了对歌单的聚类，此处将词袋模型和lda模型的相关参数分别保存在dictionary.pkl和playlist_lda.model中。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models, similarities\n",
    "import gensim\n",
    "import pickle as pkl\n",
    "\n",
    "all_sentence = []\n",
    "dic = pkl.load(open(\"dic_splited_sentence.pkl\", \"rb\"))\n",
    "for (key, value) in dic.items():\n",
    "    all_sentence.append(value)\n",
    "# 词袋模型\n",
    "dictionary = corpora.Dictionary(all_sentence)\n",
    "print(type(dictionary))\n",
    "pkl.dump(dictionary, open(\"./model/dictionary.pkl\", \"wb\"))\n",
    "\n",
    "corpus = [dictionary.doc2bow(i) for i in all_sentence]\n",
    "print(corpus[5])\n",
    "# lda训练\n",
    "lda = gensim.models.ldamodel.LdaModel(\n",
    "    corpus=corpus, id2word=dictionary, num_topics=30)\n",
    "for topic in lda.print_topics(num_topics=30, num_words=5):\n",
    "    print(topic[1])\n",
    "\n",
    "lda.save('./model/playlist_lda.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDA主题模型将所有的文本分为30个主题（类别数可自己确定，此处设置30个，解释性较好），每一个主题下有很多词，下图中每一行是一个主题，包括该主题里的词语以及出现概率。\n",
    "![lda.png](./lda.png)\n",
    "可以看到LDA主题建模的效果较好，是具有解释性的。\n",
    "\n",
    "# 3 协同过滤推荐 \n",
    "为了实现推荐，考虑使用基于用户的协同过滤方法。这一方法最初是使用用户对不同电影的评分来计算相似度，并找到与某一用户最相似的前k个用户，最终推荐的结果是该用户未看过但是与他最相似的用户看过的电影。    \n",
    "这一过程可类比到歌曲推荐：假设一个用户收藏了某一歌单， 那么在针对该用户的歌曲推荐中可以结合该收藏歌单来进行推荐。具体来说，LDA主题建模已经将歌单分为了30个类别，每一个歌单所属类别概率最高的那一类的编号（1-30）就可以作为该歌单的“类别评分”，歌单里的歌曲相当于继承了歌单的这个“类别评分”，以此代替原来协同过滤方法的用户对电影的主观评分（毕竟用户只是进行了收藏歌单行为，并没有主观地给歌曲打分，但是我们可以认为用户进行收藏行为，就是部分认可了这个歌单对歌曲的类别判断）。根据这个歌单评分，我们就能够用协同过滤算法找到与该歌单最相似的k个歌单，然后返回最相似的歌单中原歌单未出现的歌曲进行推荐。 \n",
    "推荐系统相关的算法在surprise库中有较为完整的实现，可以调用surprise库中的相关算法进行实现。\n",
    "\n",
    "## 3.1 构建符合suprise的数据类型\n",
    "读取模型和构建好的dictionary 根据这一模型，能够获得歌单最符合的主题。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = models.ldamodel.LdaModel.load('./model/playlist_lda.model')\n",
    "for topic in lda.print_topics(num_topics=30, num_words=5):\n",
    "    print(topic[1])\n",
    "\n",
    "dic = pkl.load(open(\"dic_splited_sentence.pkl\",\"rb\"))\n",
    "doc_list=dic['486555874']\n",
    "print(doc_list)\n",
    "dictionary=pkl.load(open(\"./model/dictionary.pkl\",\"rb\"))\n",
    "bow = [dictionary.doc2bow(doc_list)]\n",
    "print(bow)\n",
    "print(list(lda.get_document_topics(bow)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里就将歌曲解析成【歌单id 歌曲id 评分】的格式（surprise库处理的格式），并按照该格式 格式存入formatted_music.txt中。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#coding: utf-8\n",
    "import json\n",
    "import sys\n",
    "from gensim import corpora, models, similarities\n",
    "import pickle as pkl\n",
    "\n",
    "lda = models.ldamodel.LdaModel.load('./model/playlist_lda.model')  # 加载模型\n",
    "dic = pkl.load(open(\"dic_splited_sentence.pkl\", \"rb\"))  # 读入字典\n",
    "dictionary = pkl.load(open(\"./model/dictionary.pkl\", \"rb\"))\n",
    "\n",
    "\n",
    "def isNull(s):\n",
    "    return len(s.split(\",\")) > 2\n",
    "\n",
    "\n",
    "def getSongId(song_info):\n",
    "    # 歌曲信息：歌曲id:::歌曲名称:::歌手:::歌曲热度\n",
    "    try:\n",
    "        song = song_info.split(\":::\")\n",
    "        return song[0]\n",
    "    except Exception as e:\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def getSongScore(x):\n",
    "    doc_list = dic[x]\n",
    "    bow = [dictionary.doc2bow(doc_list)]\n",
    "    distribution = list(lda.get_document_topics(bow))\n",
    "    score = 0\n",
    "    maxp = 0\n",
    "    for i in distribution[0]:\n",
    "        if(maxp < i[1]):\n",
    "            score, maxp = i[0], i[1]\n",
    "    # print(score,maxp)\n",
    "    return score+1\n",
    "\n",
    "\n",
    "def formatInfo(in_line):\n",
    "    try:\n",
    "        contents = in_line.strip().split(\"\\t\")\n",
    "        playlist = contents[0].split(\"##\")\n",
    "        # 歌单信息playlist contents[0]： 歌单名##歌单标签##歌单id##歌单描述##歌单收藏数\n",
    "        # 歌曲信息content[1:]（每一条是以:::分割的信息）\n",
    "        songScore = getSongScore(playlist[2])\n",
    "        songs_info = map(\n",
    "            lambda x: playlist[2]+\",,\"+str(getSongId(x))+\",,\"+str(songScore), contents[1:])\n",
    "        songs_info = filter(isNull, songs_info)\n",
    "        return \"\\n\".join(songs_info)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return False\n",
    "\n",
    "\n",
    "def readFile(in_file, out_file):\n",
    "    out = open(out_file, 'w', encoding=\"utf-8\")\n",
    "    num = 0\n",
    "    for line in open(in_file, encoding=\"utf-8\"):\n",
    "        print(\"-----\", num, \"-------\")\n",
    "        num = num+1\n",
    "        result = formatInfo(line)\n",
    "        if(result):\n",
    "            out.write(result.strip()+\"\\n\")\n",
    "    out.close()\n",
    "\n",
    "\n",
    "readFile(\"./playlist_info_utf8.txt\", \"./formatted_music.txt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  3.2 使用surprise库完成推荐\n",
    "这里可以使用surprise库的协同过滤算法，并通过KNN算法求出与某一个歌单最相近的topK个歌单。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import pickle as pkl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from surprise import dataset\n",
    "from surprise import KNNBaseline, Reader\n",
    "\n",
    "id2name =pkl.load(open(\"playlist.pkl\",\"rb\"))\n",
    "name2id = {}\n",
    "for id in id2name:\n",
    "    name2id[id2name[id]] = id\n",
    "\n",
    "file_path = os.path.expanduser('./formatted_music.txt')\n",
    "reader = Reader(line_format='user item rating', sep=',')\n",
    "music = dataset.load_from_file(file_path, reader=reader)\n",
    "trainset = music.build_full_trainset() # 数据集准备\n",
    "algo = KNNBaseline() #KNN算法\n",
    "algo.train(trainset)\n",
    "\n",
    "test = name2id.keys()[39] \n",
    "test_id = name2id[test]\n",
    "test_inner_id = algo.trainset.to_inner_uid(test_id)\n",
    "print(\"歌单名称:\", test,\"歌单id:\", test_id,\"内部id:\", test_inner_id)\n",
    "playlist_neighbors = algo.get_neighbors(test_inner_id, k=10)\n",
    "\n",
    "playlist_neighbors = (algo.trainset.to_raw_uid(inner_id)\n",
    "                       for inner_id in playlist_neighbors)\n",
    "playlist_neighbors = (id2name[playlist_id]\n",
    "                       for playlist_id in playlist_neighbors)\n",
    "\n",
    "print(test, \"最接近的10个歌单为：\")\n",
    "for i in playlist_neighbors:\n",
    "    print(i, algo.trainset.to_inner_uid(name2id[i]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 实现协同过滤算法\n",
    "本来使用已经写好的surprise是非常方便的，但是surprise尽管能安装，但是无法导入，因此我决定直接自己写协同过滤的算法。  \n",
    "首先，使用formatted_music.txt，使用getData函数，将一些不符合常规的行去除；接着建立一个二维字典，并存入listsong.pkl，字典的键是歌单的原id，字典的值是一个字典，该字典的键是歌曲的原id,值是歌曲类别评分（一个歌单下的歌曲类别评分相同）。本来最开始打算建立的是一个歌单与歌曲的array，但是因为有二十多万首歌，有内存限制。  \n",
    "接着使用余弦相似度进行计算，通过getSimilarityMatrix函数获得一个大小为playlist_num（24949）维的矩阵，并存入sim_matrix.npy中。（计算相似度矩阵就花了两小时多一点，算了三亿多次相似度，过后再想想怎么优化算法）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import pickle as pkl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import time\n",
    "from math import *\n",
    "\n",
    "\n",
    "def getInnerId(dic, key):\n",
    "    # 输入:字典key 输出:key对应的内部id\n",
    "    num = 0\n",
    "    for (k, v) in dic.items():\n",
    "        if(k != key):\n",
    "            num = num+1\n",
    "        else:\n",
    "            return num\n",
    "\n",
    "\n",
    "def getDataset(in_file):\n",
    "    # 输出二维数组 每一列是 歌单id 歌曲id 类别评分\n",
    "    dataset = []\n",
    "    for line in open(in_file, encoding=\"utf-8\"):\n",
    "        line = line.strip().split(',,')  # 单逗号无法分割，古典曲目题目会有逗号\n",
    "        if(line[1].isdigit()):  # 保证歌单id都是数字，解决取不到歌曲id的问题（非法字符会出现歌曲id是歌曲名的情况）\n",
    "            dataset.append(line)\n",
    "    dataset = np.array(dataset)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def addInnerDic(dic, key_a, key_b, val):\n",
    "    # 二维字典辅助函数\n",
    "    if key_a in dic:\n",
    "        dic[key_a].update({key_b: val})\n",
    "    else:\n",
    "        dic.update({key_a: {key_b: val}})\n",
    "\n",
    "\n",
    "def getListSongDic(dataset, out_pkl):\n",
    "        # 输入：dataset 格式[歌单id,歌曲id,类别评分]\n",
    "        # 输出：out_pkl 保存了一个二维字典 {歌单：{歌曲id1：类别评分(string)，歌曲id2:类别评分....}}\n",
    "    dataset_dic = {}\n",
    "    for i in dataset:  # 2998219\n",
    "        playlist_id, songlist_id, item_score = i[0], i[1], i[2]+\".0\"\n",
    "        addInnerDic(dataset_dic, playlist_id, songlist_id, item_score)\n",
    "    pkl.dump(dataset_dic, open(out_pkl, \"wb\"))\n",
    "\n",
    "\n",
    "def getIdDic(in_pkl):\n",
    "    # 输入：pkl文件 key:value 输出：out_dic字典 内部id:原id\n",
    "    in_dic = pkl.load(open(in_pkl, \"rb\"))\n",
    "    out_dic = {}\n",
    "    id = 0\n",
    "    for (k, v) in in_dic.items():\n",
    "        out_dic[id] = k\n",
    "        id = id+1\n",
    "    return out_dic\n",
    "\n",
    "\n",
    "def getDistance(pid, list2song_dic):\n",
    "    # 输入：歌单i 歌单歌曲字典\n",
    "    # 输出：歌单i向量长度\n",
    "    # i为内部id\n",
    "    di = 0\n",
    "    if (pid not in list2song_dic):\n",
    "        return 1.0\n",
    "    for(k, v) in list2song_dic[pid].items():\n",
    "        di = di+float(v)**2\n",
    "    return sqrt(di)\n",
    "\n",
    "\n",
    "def getSimilarity(pid_i, pid_j, dis_i, dis_j, list2song_dic):\n",
    "    # 输入：歌单i（内部id） 歌单j（内部id） 歌单i向量长度 歌单j向量长度 歌单歌曲字典\n",
    "    # 输出：歌单i和歌单j的相似度\n",
    "    numerator = 0\n",
    "    for(k, v) in list2song_dic[pid_i].items():\n",
    "        if(k in list2song_dic[pid_j].keys()):\n",
    "            numerator = numerator+float(v)*float(list2song_dic[pid_j][k])\n",
    "    return numerator/(dis_i*dis_j)\n",
    "\n",
    "\n",
    "def getSimilarityMatrix(playlist_pkl, listsong_pkl):\n",
    "    # 输入：歌单字典【歌单id：歌单名】 歌单歌曲字典【歌单id:{歌曲id1:歌曲名1，歌单id2:歌曲名2...}...】\n",
    "    # 输出：相似度矩阵sim_matrix sim_matrix[i][j]存储原id为i和j的歌单的相似度\n",
    "    playlist = pkl.load(open(playlist_pkl, \"rb\"))\n",
    "    playlist_num = len(playlist)  # 歌单数 24949\n",
    "    id2pid_dic = getIdDic(playlist_pkl)\n",
    "    list2song_dic = pkl.load(open(listsong_pkl, \"rb\"))\n",
    "    sim_matrix = np.zeros((playlist_num, playlist_num),\n",
    "                          dtype=np.float)  # 相似度矩阵\n",
    "    dis_list = []  # 存储每一个歌单的向量长度\n",
    "    for i in range(0, playlist_num):\n",
    "        di = getDistance(id2pid_dic[i], list2song_dic)\n",
    "        dis_list.append(di)\n",
    "    for i in range(0, playlist_num):\n",
    "        print(i)\n",
    "        if (id2pid_dic[i] in list2song_dic):\n",
    "            for j in range(i, playlist_num):\n",
    "                if(id2pid_dic[j] in list2song_dic):\n",
    "                    sim_matrix[i][j] = getSimilarity(\n",
    "                        id2pid_dic[i], id2pid_dic[j], dis_list[i], dis_list[j], list2song_dic)\n",
    "    np.save(\"sim_matrix.npy\", sim_matrix)\n",
    "    return sim_matrix\n",
    "\n",
    "\n",
    "def topk(i, sim_matrix, k):\n",
    "    # 输入第i个歌单id 输出最接近的list的前k个歌单\n",
    "    sim_list = []\n",
    "    return sim_list\n",
    "\n",
    "\n",
    "dataset = getDataset(\"./formatted_music.txt\")\n",
    "getListSongDic(dataset, \"./listsong.pkl\")\n",
    "getSimilarityMatrix(\"./playlist.pkl\", \"./listsong.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 获取推荐结果\n",
    "使用getTopK函数，获取与歌单最相近的k个歌单，并返回最相似的歌单未出现在原歌单中的歌曲信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def getIdDic(in_pkl):\n",
    "    # 输入：pkl文件 key:value 输出：out_dic字典 内部id:原id\n",
    "    in_dic = pkl.load(open(in_pkl, \"rb\"))\n",
    "    out_dic = {}\n",
    "    id = 0\n",
    "    for (k, v) in in_dic.items():\n",
    "        out_dic[id] = k\n",
    "        id = id+1\n",
    "    return out_dic\n",
    "\n",
    "\n",
    "def getTopK(pid, sim_matrix, k, playlist_pkl):\n",
    "    # 输入：歌单内部id 相似度矩阵 取相似的歌单数k 歌单字典 歌单原id:歌单名\n",
    "    # 输出：打印前k个最相似的歌单原id、歌单名和相似度 并返回最相似的歌单\n",
    "    playlist = pkl.load(open(playlist_pkl, \"rb\"))\n",
    "    id2pid_dic = getIdDic(playlist_pkl)\n",
    "    result_dic = {}\n",
    "    for i in range(0, pid):\n",
    "        sim_matrix[pid][i] = sim_matrix[i][pid]  # 补齐矩阵\n",
    "    result_list = sim_matrix[pid].tolist()\n",
    "    for i in range(0, len(result_list)):\n",
    "        result_dic[id2pid_dic[i]] = result_list[i]  # result_dic 歌单原id:相似度\n",
    "    sorted_result = []\n",
    "    sorted_sim_value = []\n",
    "    for i in sorted(result_dic, key=result_dic.__getitem__, reverse=True):\n",
    "        sorted_result.append(i)\n",
    "        sorted_sim_value.append(result_dic[i])\n",
    "    print(\"与\", id2pid_dic[pid], \":\", playlist[id2pid_dic[pid]], \" 最相似的歌单为:\")\n",
    "    for i in range(1, k+1):\n",
    "        print(sorted_result[i], \":\", playlist[sorted_result[i]],\n",
    "              \" 相似度为:\", sorted_sim_value[i])  # 歌曲id：歌曲名 相似度\n",
    "    return sorted_result[1]  # 除了本身的最接近的歌单\n",
    "\n",
    "\n",
    "def getRecommendSong(top1, pid, playlist_pkl, song_pkl, listsong_pkl):\n",
    "    # 打印最相似歌单中未出现在原歌单的歌曲id 歌曲名 创作者\n",
    "    print(\"推荐以下歌曲：\")\n",
    "    song = pkl.load(open(song_pkl, \"rb\"))\n",
    "    id2pid_dic = getIdDic(playlist_pkl)\n",
    "    list2song_dic = pkl.load(open(listsong_pkl, \"rb\"))\n",
    "    for (k, v) in list2song_dic[top1].items():\n",
    "        if (k not in list2song_dic[id2pid_dic[pid]]):\n",
    "            print(k, \":\", song[k])\n",
    "\n",
    "\n",
    "sim_matrix = np.load(\"sim_matrix.npy\")\n",
    "for i in range(0, 5):\n",
    "    top1 = getTopK(i, sim_matrix, 10, \"./playlist.pkl\")\n",
    "    getRecommendSong(top1, i, \"./playlist.pkl\", \"./song.pkl\", \"./listsong.pkl\")\n",
    "    print(\"---------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 结果与小结\n",
    "## 4.1 推荐结果\n",
    "\n",
    "![playlist_sim.png](./playlist_sim.png)\n",
    "\n",
    "---\n",
    "\n",
    "![recommend.png](./recommend.png)\n",
    "通过上面的两张图，还是能直观地感觉这样的推荐是靠谱的，但是在衡量准确性的时候会产生一个问题：实际的评分是一个连续值，能够通过算法求出一个估计值，如果有测试数据集就可以计算出实际的评分和估计值的均方根误差等来衡量。但是此处使用的类别评分本质上是一个离散值，难以进行估计值的求解，况且也没有测试集，因此这里就没有进行准确性的衡量。  \n",
    "但是从协同过滤的整体思想来看，引入LDA主题建模计算出来的类别评分，也是在语言的层面上借助了人类的集体智慧，理论上在实际使用中应该会让人“感觉”推荐得更准。（也不一定，其实还是应该想想怎么衡量准确性）\n",
    "\n",
    "## 4.2 协同过滤无法解决的问题\n",
    "1.数据的稀疏性。衡量歌单的相似性时会发现，数据集中一共有二十多万首歌，但一般一个歌单的数量一般不会超过1000首，这就会造成歌单里的歌曲重叠性较低，因此从结果也能够看出，即使是最接近的歌单，其和原歌单的相似度也较低。  \n",
    "2.另外，当数据量较大时，协同过滤需要花费一定的时间，同时扩展性也更差，当新增了用户或者歌曲时，需要重新计算，难以扩展。  \n",
    "## 4.3 未来的工作\n",
    "\n",
    "1.考虑如何衡量引入了LDA主题建模的协同过滤的准确率。  \n",
    "2.学习和实现冷启动问题的解决方案。  \n",
    "3.学习和思考算法的效率优化。\n",
    "4.使用word2vec类比到song2vec的推荐。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "166.8px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
