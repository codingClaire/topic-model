{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 融合LDA主题模型的歌单相似度和歌曲推荐\n",
    "\n",
    "## 基于歌单的相似度进行推荐\n",
    "\n",
    "###  数据预处理\n",
    "\n",
    "#### 筛选需要的数据项\n",
    "\n",
    "首先对15G的歌单数据进行处理，选取了数据集中歌单的信息，包括歌单名、歌单标签、歌单id、歌单描述、歌单收藏数、歌曲信息，每一个歌单下包含着多首歌曲，每一首歌曲的信息包括歌曲id、歌曲名称、歌手、歌曲热度。  \n",
    "（15G的数据量较大，在jupyter跑不起来，python文件可以）  \n",
    "最终输出一个playlist_info的txt文件  \n",
    "经过筛选 只剩下24951个歌单"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import pickle as pk\n",
    "import sys\n",
    "import json\n",
    "\n",
    "def readPlaylist(in_line):\n",
    "    data = json.loads(in_line)\n",
    "    name = data['result']['name'] #歌单名称\n",
    "    tags = \",\".join(data['result']['tags']) # 标签以逗号分隔的字符串存储\n",
    "    subscribed_count = data['result']['subscribedCount'] # 收藏数\n",
    "    try:\n",
    "        description=data['result']['description'] #歌单的描述\n",
    "    except Exception as e:\n",
    "        description=\" \"\n",
    "    if(subscribed_count<100): # 筛选小于100收藏的歌单\n",
    "        return False\n",
    "    playlist_id = data['result']['id'] # 歌单的id名\n",
    "    song_info = ''\n",
    "    songs = data['result']['tracks'] # 歌曲\n",
    "    for song in songs: \n",
    "        try:\n",
    "            song_info += \"\\t\"+\":::\".join([str(song['id']),song['name'],song['artists'][0]['name'],str(song['popularity'])])\n",
    "            song_info=song_info.replace(\"\\n\",\" \")\n",
    "            # 歌曲信息：歌曲id:::歌曲名称:::歌手:::歌曲热度\n",
    "        except Exception as e:\n",
    "            continue\n",
    "     # 歌单信息： 歌单名##歌单标签##歌单id##歌单描述##歌单收藏数##歌曲信息（以:::分割的信息）\n",
    "    final_info=str(name+\"##\"+tags+\"##\"+str(playlist_id)+\"##\"+str(description)+\"##\"+str(subscribed_count)+song_info)\n",
    "    return final_info.replace(\"\\n\",\" \")\n",
    "   \n",
    "def readFile(in_file, out_file):\n",
    "    out = open(out_file, 'w',encoding=\"utf-8\")\n",
    "    num=0\n",
    "    for line in open(in_file,encoding='utf-8'):\n",
    "        result = readPlaylist(line)\n",
    "        #print(result)\n",
    "        num=num+1\n",
    "        print(\"------------------\",num,\"--------------------\")\n",
    "        if(result):\n",
    "            out.write(str(result)+\"\\n\")  \n",
    "    out.close()\n",
    "    \n",
    "if __name__==\"__main__\":\n",
    "    readFile(\"./playlistdetail.all.json\", \"./playlist_info_utf8.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 构建歌曲（歌单）id和名称的字典\n",
    "\n",
    "在这一步构建了歌曲和歌单的两个属性id和名称对应的字典。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pk\n",
    "import sys\n",
    "\n",
    "def getDict(i, dic_playlist, dic_song):\n",
    "    contents = i.strip().split(\"\\t\")\n",
    "    playlist = contents[0].split(\"##\")\n",
    "    # 歌单信息： 歌单名##歌单标签##歌单id##歌单描述##歌单收藏数##歌曲信息（以:::分割的信息）\n",
    "    dic_playlist[playlist[2]] = playlist[0]\n",
    "    for i in contents[1:]:\n",
    "        try:\n",
    "            song_info = i.split(\":::\")\n",
    "            dic_song[song_info[0]] = song_info[1]+\"\\t\"+song_info[2]\n",
    "        except:\n",
    "            print(\"error\"+i+\"\\n\")\n",
    "\n",
    "\n",
    "def readFile(in_file, out_playlist, out_song):\n",
    "    dic_playlist = {}  # dic_playlist[歌单id]=歌单名称\n",
    "    dic_song = {}  # dic_song[歌曲id]=歌曲名称\n",
    "    num = 0\n",
    "    for i in open(in_file, encoding=\"utf-8\"):\n",
    "        num = num+1\n",
    "        print(\"------------------\", num, \"--------------------\")\n",
    "        getDict(i, dic_playlist, dic_song)\n",
    "    pk.dump(dic_playlist, open(out_playlist, \"wb\"))\n",
    "    pk.dump(dic_song, open(out_song, \"wb\"))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    readFile(\"./playlist_info_utf8.txt\", \"playlist.pkl\", \"song.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 歌单文本信息分词（utf-8存储）\n",
    "为了后续的LDA主题模型，对歌单的文本信息进行分词。\n",
    "歌单的文本信息包括歌单名称、歌单标签和歌单描述。歌单的文本信息中除了中英文，还会有很多表情符（使用unicode编码），虽然表情符一定程度上可以表征歌单的情感以及歌曲的情感，当然要具体细究那就是后续的工作了，此处需要将这些表情符进行过滤。  \n",
    "1.将需要使用的文本信息进行提取，提取后存储在utf8_playlist.txt中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import pickle as pk\n",
    "import sys\n",
    "import json\n",
    "\n",
    "def readPlaylist(in_line):\n",
    "    data = json.loads(in_line)\n",
    "    name = data['result']['name'] #歌单名称\n",
    "    tags = \",\".join(data['result']['tags']) # 标签以逗号分隔的字符串存储\n",
    "    try:\n",
    "        description=data['result']['description'] #歌单的描述\n",
    "    except Exception as e:\n",
    "        description=\"---\"\n",
    "    description=(str(description)).replace(\"\\n\",\"  \")\n",
    "    subscribed_count = data['result']['subscribedCount'] # 收藏数\n",
    "    if(subscribed_count<100):\n",
    "        return False\n",
    "    playlist_id = data['result']['id'] # 歌单的id名\n",
    "    return name+\"##\"+tags+\"##\"+str(playlist_id)+\"##\"+str(description)\n",
    "    # 歌单信息： 歌单名##歌单标签##歌单id##歌单描述\n",
    "\n",
    "def readFile(in_file, out_file):\n",
    "    out=open(out_file, 'w',encoding=\"utf-8\")\n",
    "    num=0\n",
    "    for line in open(in_file,encoding='utf-8'):\n",
    "        result=readPlaylist(line)\n",
    "        num=num+1\n",
    "        print(\"------------------\",num,\"--------------------\")\n",
    "        # print(result)\n",
    "        if(result):\n",
    "            out.write(str(result)+\"\\n\")  \n",
    "    out.close()\n",
    "    \n",
    "if __name__==\"__main__\":\n",
    "    readFile(\"D:\\playlistdetail.all.json\", \"./utf8_playlist.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.将所有的文本信息（标签、名称、描述）合在一起，由此汇成一个“文档”，这一个文档可认为是对歌单的总体描述，然后使用jieba分词，每一个文档分词的结果用list存储起来，最后歌单id和分词后的list以字典形式存储在dic_splited_sentence.pkl。  \n",
    "（这里也可以考虑加入歌单里的每一首歌的名称，甚至歌的歌词进行主题建模，但由于收集歌词的难度较大，就没有做到这一步）  \n",
    "由于歌单的描述中会混合非中文的其他语言文字或者一些颜文字、表情等，为了简便分词，将这些词都过滤，并且也根据导入的停用词表，过滤停用词，在这里会根据后面LDA模型的生成情况，添加一些针对歌单描述的特定停用词，如“歌曲”、“专辑”、“歌手”、“年度”、“单曲”、“歌单”等。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "import chardet\n",
    "import jieba\n",
    "import pickle as pk\n",
    "import re\n",
    "import emoji\n",
    "import pandas as pd\n",
    "\n",
    "def getSplitedSentence(i, dic_splited_sentence, stopwords):\n",
    "    # 获取歌单信息: 歌单名##歌单标签##歌单id##歌单描述\n",
    "    contents = i.strip().split(\"\\t\")\n",
    "    playlist = contents[0].split(\"##\")\n",
    "    sentence = \" \".join([playlist[0], playlist[1], playlist[3]])\n",
    "    chinese_sentence = re.sub('[^\\u4e00-\\u9fa5]', '', sentence)  # 过滤非中文\n",
    "    word_list = jieba.lcut(chinese_sentence)\n",
    "    word_list = filter(lambda x: len(x) > 1, word_list)\n",
    "    word_list = filter(lambda x: x not in stopwords, word_list)  # 过滤停用词\n",
    "    word_list = list(word_list)\n",
    "    # print(word_list)\n",
    "    dic_splited_sentence[playlist[2]] = word_list\n",
    "\n",
    "\n",
    "def readFile(in_file, out_pk, stopwords):\n",
    "    dic_splited_sentence = {}\n",
    "    num = 0\n",
    "    for i in open(in_file, encoding=\"utf-8\"):\n",
    "        print(\"------------\", num, \"-------------\")\n",
    "        num = num+1\n",
    "        getSplitedSentence(i, dic_splited_sentence, stopwords)\n",
    "    pk.dump(dic_splited_sentence, open(out_pk, \"wb\"))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    stopwords = pd.read_csv(\"./stopwords.txt\", index_col=False,\n",
    "                            quoting=3, sep=\"\\t\", names=['stopword'], encoding='utf-8')\n",
    "    stopwords = stopwords['stopword'].values\n",
    "    readFile(\"./playlist_info_utf8.txt\",\n",
    "             \"./dic_splited_sentence.pkl\", stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 歌单文本信息LDA主题建模\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models, similarities\n",
    "import gensim\n",
    "import pickle as pkl\n",
    "\n",
    "all_sentence = []\n",
    "dic = pkl.load(open(\"dic_splited_sentence.pkl\", \"rb\"))\n",
    "for (key, value) in dic.items():\n",
    "    all_sentence.append(value)\n",
    "# 词袋模型\n",
    "dictionary = corpora.Dictionary(all_sentence)\n",
    "print(type(dictionary))\n",
    "pkl.dump(dictionary, open(\"./model/dictionary.pkl\", \"wb\"))\n",
    "\n",
    "corpus = [dictionary.doc2bow(i) for i in all_sentence]\n",
    "print(corpus[5])\n",
    "# lda训练\n",
    "lda = gensim.models.ldamodel.LdaModel(\n",
    "    corpus=corpus, id2word=dictionary, num_topics=30)\n",
    "for topic in lda.print_topics(num_topics=30, num_words=5):\n",
    "    print(topic[1])\n",
    "\n",
    "lda.save('./model/playlist_lda.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 协同过滤推荐\n",
    "在进行了LDA建模后，可以看到建模效果较好，因此考虑使用基于用户的协同过滤算法实现推荐。  \n",
    "这样解决的问题是，作为用户，当我听到了一个很喜欢的歌单时，我可以通过相似推荐得到与这个歌单相似的歌曲。 \n",
    "这里具体的实现使用suprise库\n",
    "#### 构建符合suprise的数据类型\n",
    "读取模型和构建好的dictionary 根据这一模型 能够获得每一个歌单最符合的主题。\n",
    "![](./pic1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = models.ldamodel.LdaModel.load('./model/playlist_lda.model')\n",
    "for topic in lda.print_topics(num_topics=30, num_words=5):\n",
    "    print(topic[1])\n",
    "\n",
    "dic = pkl.load(open(\"dic_splited_sentence.pkl\",\"rb\"))\n",
    "doc_list=dic['486555874']\n",
    "print(doc_list)\n",
    "dictionary=pkl.load(open(\"./model/dictionary.pkl\",\"rb\"))\n",
    "bow = [dictionary.doc2bow(doc_list)]\n",
    "print(bow)\n",
    "print(list(lda.get_document_topics(bow)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里就将歌曲解析成 歌单id 歌曲id 评分 的格式  \n",
    "由于借用的是基于用户的协同过滤，那么歌单之间的相似性通过评分来决定，这个评分就是LDA模型分出的30种类别号。\n",
    "将每一首歌按照格式存入formatted_music.txt中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#coding: utf-8\n",
    "import json\n",
    "import sys\n",
    "from gensim import corpora, models, similarities\n",
    "import pickle as pkl\n",
    "\n",
    "lda = models.ldamodel.LdaModel.load('./model/playlist_lda.model')  # 加载模型\n",
    "dic = pkl.load(open(\"dic_splited_sentence.pkl\", \"rb\"))  # 读入字典\n",
    "dictionary = pkl.load(open(\"./model/dictionary.pkl\", \"rb\"))\n",
    "\n",
    "\n",
    "def isNull(s):\n",
    "    return len(s.split(\",\")) > 2\n",
    "\n",
    "\n",
    "def getSongId(song_info):\n",
    "    # 歌曲信息：歌曲id:::歌曲名称:::歌手:::歌曲热度\n",
    "    try:\n",
    "        song = song_info.split(\":::\")\n",
    "        return song[0]\n",
    "    except Exception as e:\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def getSongScore(x):\n",
    "    doc_list = dic[x]\n",
    "    bow = [dictionary.doc2bow(doc_list)]\n",
    "    distribution = list(lda.get_document_topics(bow))\n",
    "    score = 0\n",
    "    maxp = 0\n",
    "    for i in distribution[0]:\n",
    "        if(maxp < i[1]):\n",
    "            score, maxp = i[0], i[1]\n",
    "    # print(score,maxp)\n",
    "    return score\n",
    "\n",
    "\n",
    "def formatInfo(in_line):\n",
    "    try:\n",
    "        contents = in_line.strip().split(\"\\t\")\n",
    "        playlist = contents[0].split(\"##\")\n",
    "        # 歌单信息playlist contents[0]： 歌单名##歌单标签##歌单id##歌单描述##歌单收藏数\n",
    "        # 歌曲信息content[1:]（每一条是以:::分割的信息）\n",
    "        songScore = getSongScore(playlist[2])\n",
    "        songs_info = map(\n",
    "            lambda x: playlist[2]+\",\"+str(getSongId(x))+\",\"+str(songScore), contents[1:])\n",
    "        songs_info = filter(isNull, songs_info)\n",
    "        return \"\\n\".join(songs_info)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return False\n",
    "\n",
    "\n",
    "def readFile(in_file, out_file):\n",
    "    out = open(out_file, 'w', encoding=\"utf-8\")\n",
    "    num = 0\n",
    "    for line in open(in_file, encoding=\"utf-8\"):\n",
    "        print(\"-----\", num, \"-------\")\n",
    "        num = num+1\n",
    "        result = formatInfo(line)\n",
    "        if(result):\n",
    "            out.write(result.strip()+\"\\n\")\n",
    "    out.close()\n",
    "\n",
    "\n",
    "readFile(\"./playlist_info_utf8.txt\", \"./formatted_music.txt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  协同过滤算法\n",
    "本来使用已经写好的surprise是非常方便的，但是surprise在我的两台电脑都跑不了，绝了，我决定手写。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import pickle as pkl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from surprise import dataset\n",
    "from surprise import KNNBaseline, Reader\n",
    "\n",
    "id2name =pkl.load(open(\"playlist.pkl\",\"rb\"))\n",
    "name2id = {}\n",
    "for id in id2name:\n",
    "    name2id[id2name[id]] = id\n",
    "\n",
    "file_path = os.path.expanduser('./formatted_music.txt')\n",
    "reader = Reader(line_format='user item rating', sep=',')\n",
    "music = dataset.load_from_file(file_path, reader=reader)\n",
    "trainset = music.build_full_trainset() # 数据集准备\n",
    "algo = KNNBaseline() #KNN算法\n",
    "algo.train(trainset)\n",
    "\n",
    "#取一个歌单试试\n",
    "test = name2id.keys()[39] \n",
    "test_id = name2id[test]\n",
    "test_inner_id = algo.trainset.to_inner_uid(test_id)\n",
    "print(\"歌单名称:\", test,\"歌单id:\", test_id,\"内部id:\", test_inner_id)\n",
    "playlist_neighbors = algo.get_neighbors(test_inner_id, k=10)\n",
    "\n",
    "playlist_neighbors = (algo.trainset.to_raw_uid(inner_id)\n",
    "                       for inner_id in playlist_neighbors)\n",
    "playlist_neighbors = (id2name[playlist_id]\n",
    "                       for playlist_id in playlist_neighbors)\n",
    "\n",
    "print(test, \"最接近的10个歌单为：\\n\")\n",
    "for i in playlist_neighbors:\n",
    "    print(i, algo.trainset.to_inner_uid(name2id[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "316.4px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
